{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyATLU4z1cYj"
      },
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "outputs": [],
      "source": [
        "#import necessary package\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2U2_Q5ebos3"
      },
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "**[TO DO: Insert Answer here]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd3_QIXp5j_2"
      },
      "source": [
        "***Why we need to hand the data on the same scale***\n",
        "\n",
        "- The reason why we standardize or normalize data is :\n",
        "            \n",
        "            - Avoid bias in algorithms; it prevents features with larger scales from dominating the learning process and biases the model\n",
        "\n",
        "            - Improves convergence; it helps optimization algorithms like gradient descent to converge faster and more reliably\n",
        "\n",
        "            - Interpretability and Comparability; standardized data makes it easier to interpret feature importance and compare their effects\n",
        "\n",
        "            - Regularization and Stability; standardization stabilizes regularization techniques and prevents features from disproportionaely affecting regulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f30e2c-5848-4707-bb7f-a118fb516961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ],
      "source": [
        "# Calculate the mean\n",
        "mean = np.mean(data, axis=0)\n",
        "\n",
        "# Calculate the standard deviation\n",
        "std_dv = np.std(data, axis=0)\n",
        "\n",
        "# Standardize the data\n",
        "standardized_data = (data - mean) / std_dv\n",
        "\n",
        "print(standardized_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      },
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuhux3UEcBgw"
      },
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "7bacd7f0-1123-4f2c-cd3f-2e540ee2e786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Covariance Matrix: \n",
            "[[ 1.2        -0.42098785 -1.0835838   0.90219291 -0.37000528]\n",
            " [-0.42098785  1.2         0.20397003 -0.77149364  1.18751836]\n",
            " [-1.0835838   0.20397003  1.2        -0.59947269  0.22208218]\n",
            " [ 0.90219291 -0.77149364 -0.59947269  1.2        -0.70017993]\n",
            " [-0.37000528  1.18751836  0.22208218 -0.70017993  1.2       ]]\n"
          ]
        }
      ],
      "source": [
        "# use `ddof = 1` if using sample data (default assumption) and use `ddof = 0` if using population data\n",
        "\n",
        "cov_matrix = np.cov(standardized_data, ddof=1, rowvar=False)\n",
        "\n",
        "\n",
        "print(f\"\\nCovariance Matrix: \\n{cov_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXNcG4AFcT08"
      },
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92624876-8a63-4eee-8be0-27e7ed5e98d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Eigenvalues: \n",
            "[3.80985761e+00 1.73655615e+00 4.94531029e-02 4.74189469e-05\n",
            " 4.04085720e-01]\n",
            "\n",
            "Eigenvectors: \n",
            "[[-0.4640131   0.45182808 -0.70733581  0.28128049 -0.03317471]\n",
            " [ 0.45019005  0.48800851  0.29051532  0.6706731  -0.15803498]\n",
            " [ 0.37929082 -0.55665017 -0.48462321  0.24186072 -0.5029143 ]\n",
            " [-0.4976889   0.03162214  0.36999674 -0.03373724 -0.78311558]\n",
            " [ 0.43642295  0.49682965 -0.20861365 -0.64143906 -0.32822489]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix) #TO DO: Insert code here for eigenvalues, eigenvectors\n",
        "\n",
        "print(f\"\\nEigenvalues: \\n{eigenvalues}\")\n",
        "print(f\"\\nEigenvectors: \\n{eigenvectors}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pWho88fcbJA"
      },
      "source": [
        "### Step 4: Sort the Principal Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "d8505a35-474f-4012-a0e7-4b229586b2a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [0 1 4 2 3]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[3.80985761e+00 1.73655615e+00 4.04085720e-01 4.94531029e-02\n",
            " 4.74189469e-05]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.4640131   0.45182808 -0.03317471 -0.70733581  0.28128049]\n",
            " [ 0.45019005  0.48800851 -0.15803498  0.29051532  0.6706731 ]\n",
            " [ 0.37929082 -0.55665017 -0.5029143  -0.48462321  0.24186072]\n",
            " [-0.4976889   0.03162214 -0.78311558  0.36999674 -0.03373724]\n",
            " [ 0.43642295  0.49682965 -0.32822489 -0.20861365 -0.64143906]]\n"
          ]
        }
      ],
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance] # sort the columns\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1nILNGxpTJB"
      },
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "[We order eigenvalues and eigenvectors to identify the most significant principal components ]\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "[No, we prioritize the highest eigenvalues as they represent the principal components capturing the most variance, essential for preserving information in techniques like PCS]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWqFGNeNvgEB"
      },
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "42e004f1-5354-40bb-b92a-55ce71466e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['63.50%', '28.94%', '6.73%', '0.82%', '0.00%']\n"
          ]
        }
      ],
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "#TO DO: Insert code here\n",
        "explained_variance = (sorted_eigenvalues / np.sum(sorted_eigenvalues)) * 100\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print( explained_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB7H4InbfKx5"
      },
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "outputs": [],
      "source": [
        "k = 2\n",
        "\n",
        "reduced_data = np.matmul(standardized_data, sorted_eigenvectors[:,:k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "d26208fd-5aaf-4340-bd0b-35d23b288e75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.3577116  -0.75728867]\n",
            " [-2.27171739 -1.81970663]\n",
            " [ 1.21259114 -0.50390931]\n",
            " [-1.41935914  1.9229856 ]\n",
            " [ 1.61562536  0.87541857]\n",
            " [-1.49485157  0.28250044]]\n"
          ]
        }
      ],
      "source": [
        "print(reduced_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "53b2be03-7c78-4b8a-826f-cbe090e86526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 2)\n"
          ]
        }
      ],
      "source": [
        "print(reduced_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxQ8lTunauMQ"
      },
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "[\n",
        "    **Benefits (Positive Effects):**\n",
        "\n",
        "1. Dimensionality Reduction: PCA helps in reducing the dimensionality of data by transforming it into a lower-dimensional space while retaining most of the variance. This reduction can lead to faster computation times and reduced storage requirements.\n",
        "2. Feature Combination: PCA can combine correlated features into fewer uncorrelated components (principal components). This feature combination can help in simplifying the dataset and improving model performance by reducing multicollinearity.\n",
        "\n",
        "**Limitations (Negative Effects):**\n",
        "\n",
        "1. Loss of Interpretability: After applying PCA, the new components (principal components) might not have direct physical or intuitive meaning like the original features. This loss of interpretability can make it challenging to explain the underlying factors driving the data.\n",
        "2. Information Loss: PCA aims to capture the maximum variance in the data, but this can lead to information loss, especially if the lower-dimensional representation does not retain enough information to fully represent the original dataset. It's important to balance dimensionality reduction with retaining sufficient information for meaningful analysis.]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}